{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuRoch7cIqrYwze1smITvJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopinathak-geek/Image-classification-with-pytorch/blob/main/Titanic_spaceship.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "8vsDr22aPyOQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler    \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_df = pd.read_csv('train.csv')\n",
        "testing_df = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "VV4T1penVz2C"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_training_data(training_data):\n",
        "    df = training_data.copy()\n",
        "    df = df.dropna()\n",
        "    numericals = []\n",
        "    for i in range(len(df.dtypes)):\n",
        "        coltype = df.dtypes[i]\n",
        "        if (coltype != object): \n",
        "            numericals.append(df.iloc[:, i])\n",
        "    df = pd.DataFrame (numericals).T\n",
        "    return df\n",
        "\n",
        "def clean_testing_data(testing_data):\n",
        "    df = testing_data.copy()\n",
        "    cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
        "    for col in cols:\n",
        "        df[col].fillna(df[col].median(), inplace=True)\n",
        "    numericals = []\n",
        "    for i in range(len(df.dtypes)):\n",
        "        coltype = df.dtypes[i]\n",
        "        if (coltype != object): \n",
        "            numericals.append(df.iloc[:, i])\n",
        "    df = pd.DataFrame (numericals).T\n",
        "    return df"
      ],
      "metadata": {
        "id": "qQglOCmnWVPT"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_df = clean_training_data(training_df)\n",
        "testing_data_df = clean_testing_data(testing_df)\n",
        "test_ids = testing_df['PassengerId']\n",
        "\n",
        "training_data_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "5hjsU0dVWYqB",
        "outputId": "e19322d5-7e7b-4207-d8d8-284d8e6c2656"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Age  RoomService  FoodCourt  ShoppingMall     Spa  VRDeck  Transported\n",
              "0     39.0          0.0        0.0           0.0     0.0     0.0          0.0\n",
              "1     24.0        109.0        9.0          25.0   549.0    44.0          1.0\n",
              "2     58.0         43.0     3576.0           0.0  6715.0    49.0          0.0\n",
              "3     33.0          0.0     1283.0         371.0  3329.0   193.0          0.0\n",
              "4     16.0        303.0       70.0         151.0   565.0     2.0          1.0\n",
              "...    ...          ...        ...           ...     ...     ...          ...\n",
              "8688  41.0          0.0     6819.0           0.0  1643.0    74.0          0.0\n",
              "8689  18.0          0.0        0.0           0.0     0.0     0.0          0.0\n",
              "8690  26.0          0.0        0.0        1872.0     1.0     0.0          1.0\n",
              "8691  32.0          0.0     1049.0           0.0   353.0  3235.0          0.0\n",
              "8692  44.0        126.0     4688.0           0.0     0.0    12.0          1.0\n",
              "\n",
              "[6606 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1e329c90-0e26-4ad0-888b-9463ef50ad54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Transported</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>58.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>3576.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6715.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>371.0</td>\n",
              "      <td>3329.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16.0</td>\n",
              "      <td>303.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>41.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6819.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1643.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1872.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1049.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>353.0</td>\n",
              "      <td>3235.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>44.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>4688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6606 rows Ã— 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e329c90-0e26-4ad0-888b-9463ef50ad54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1e329c90-0e26-4ad0-888b-9463ef50ad54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1e329c90-0e26-4ad0-888b-9463ef50ad54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = training_data_df.drop(['Transported'], axis=1)\n",
        "y = training_data_df['Transported']"
      ],
      "metadata": {
        "id": "TbSyq7NBWbl4"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=69)"
      ],
      "metadata": {
        "id": "N7Uf0ClFWhXC"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "X_k_test = scaler.transform(testing_data_df)"
      ],
      "metadata": {
        "id": "Yz5ruec0WiO7"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 500\n",
        "BATCH_SIZE = 1000\n",
        "LEARNING_RATE = 0.001"
      ],
      "metadata": {
        "id": "bzkiif8vWlOt"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTfN9LZEzIyN",
        "outputId": "013c57ba-e69a-41ea-c6c4-e3268d093899"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.9841733 , -0.34938003, -0.28080437, -0.30412253, -0.27767595,\n",
              "        -0.26928524],\n",
              "       [-0.8869437 , -0.34938003, -0.28080437, -0.30412253, -0.27767595,\n",
              "        -0.26928524],\n",
              "       [ 2.47332195, -0.34938003, -0.28080437, -0.30412253, -0.27767595,\n",
              "        -0.26928524],\n",
              "       ...,\n",
              "       [ 0.2102859 , -0.34938003,  2.16651367,  0.33793811,  0.52421425,\n",
              "        -0.26755473],\n",
              "       [-0.40690575, -0.34938003, -0.28080437, -0.30412253,  0.37318561,\n",
              "        -0.2000648 ],\n",
              "       [ 0.27886275, -0.34938003,  2.32505887, -0.30412253,  3.4108392 ,\n",
              "        -0.26928524]])"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.FloatTensor(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFYcfVM6zIcl",
        "outputId": "354c94d2-8c2e-4601-d9c2-6923985f316a"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.9842, -0.3494, -0.2808, -0.3041, -0.2777, -0.2693],\n",
              "        [-0.8869, -0.3494, -0.2808, -0.3041, -0.2777, -0.2693],\n",
              "        [ 2.4733, -0.3494, -0.2808, -0.3041, -0.2777, -0.2693],\n",
              "        ...,\n",
              "        [ 0.2103, -0.3494,  2.1665,  0.3379,  0.5242, -0.2676],\n",
              "        [-0.4069, -0.3494, -0.2808, -0.3041,  0.3732, -0.2001],\n",
              "        [ 0.2789, -0.3494,  2.3251, -0.3041,  3.4108, -0.2693]])"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## train data\n",
        "class TrainData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "\n",
        "train_data = TrainData(torch.FloatTensor(X_train), torch.FloatTensor(y_train))"
      ],
      "metadata": {
        "id": "FnkvcZp0y_u0"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvKPotUsGhzU",
        "outputId": "d2ce2281-f221-4588-b418-78732de6f25a"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-1.9842, -0.3494, -0.2808, -0.3041, -0.2777, -0.2693]), tensor(1.))"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = TrainData(torch.FloatTensor(X_test), torch.FloatTensor(y_test.to_numpy()))"
      ],
      "metadata": {
        "id": "uqCaZI39FB63"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## test data    \n",
        "class TestData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data):\n",
        "        self.X_data = X_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "    \n",
        "\n",
        "k_test_data = TestData(torch.FloatTensor(X_k_test))"
      ],
      "metadata": {
        "id": "k_ZI8YjCWoe0"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
        "k_test_loader = DataLoader(dataset=k_test_data, batch_size=1)"
      ],
      "metadata": {
        "id": "SpHnlKn9WtNZ"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryClassification, self).__init__()\n",
        "        # Number of input features is 12.\n",
        "        self.layer_1 = nn.Linear(6, 12) \n",
        "        self.layer_2 = nn.Linear(12, 24)\n",
        "        self.layer_3 = nn.Linear(24, 48)\n",
        "        self.layer_4 = nn.Linear(48, 12)\n",
        "        self.layer_5 = nn.Linear(12, 6)\n",
        "        self.layer_out = nn.Linear(6, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(12)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(24)\n",
        "        self.batchnorm3 = nn.BatchNorm1d(48)\n",
        "        self.batchnorm4 = nn.BatchNorm1d(12)\n",
        "        self.batchnorm5 = nn.BatchNorm1d(6)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer_3(x))\n",
        "        x = self.batchnorm3(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer_4(x))\n",
        "        x = self.batchnorm4(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer_5(x))\n",
        "        x = self.batchnorm5(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "gmuD_xj_WwLv"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOkW5TpdWzaR",
        "outputId": "89658a5d-1926-4c47-97e4-cdf0f78fffe2"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BinaryClassification()\n",
        "model.to(device)\n",
        "print(model)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-lmfH8XW2Du",
        "outputId": "bdd077b6-c13d-412b-db8b-39b08549ccf5"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BinaryClassification(\n",
            "  (layer_1): Linear(in_features=6, out_features=12, bias=True)\n",
            "  (layer_2): Linear(in_features=12, out_features=24, bias=True)\n",
            "  (layer_3): Linear(in_features=24, out_features=48, bias=True)\n",
            "  (layer_4): Linear(in_features=48, out_features=12, bias=True)\n",
            "  (layer_5): Linear(in_features=12, out_features=6, bias=True)\n",
            "  (layer_out): Linear(in_features=6, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm3): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm4): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm5): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc"
      ],
      "metadata": {
        "id": "Ej_ZNhJXW6zR"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_history = []\n",
        "train_acc_history = []\n",
        "\n",
        "val_loss_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "for e in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    epoch_loss_train = 0\n",
        "    epoch_acc_train = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        y_pred = model(X_batch)\n",
        "        \n",
        "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
        "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss_train += loss.item()\n",
        "        epoch_acc_train += acc.item()\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      epoch_loss_val = 0\n",
        "      epoch_acc_val = 0\n",
        "\n",
        "      for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        y_pred = model(X_batch)\n",
        "        \n",
        "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
        "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
        "\n",
        "        epoch_loss_val += loss.item()\n",
        "        epoch_acc_val += acc.item()\n",
        "\n",
        "        \n",
        "    train_loss_history.append(epoch_loss_train/len(train_loader))\n",
        "    train_acc_history.append(epoch_acc_train/len(train_loader))\n",
        "\n",
        "    val_loss_history.append(epoch_loss_val/len(test_loader))\n",
        "    val_acc_history.append(epoch_acc_val/len(test_loader))\n",
        "    print(f'Epoch {e+0:03}: | Training Loss: {epoch_loss_train/len(train_loader):.5f} | validation Loss: {epoch_loss_val/len(test_loader):.5f} | Training Acc: {epoch_acc_train/len(train_loader):.3f} | validation Acc: {epoch_acc_val/len(test_loader):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf9E5ngoW92C",
        "outputId": "4a1c7a3b-65bb-4310-9c3f-8bdca000ea48"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001: | Training Loss: 0.69265 | validation Loss: 0.70212 | Training Acc: 56.500 | validation Acc: 50.303\n",
            "Epoch 002: | Training Loss: 0.65615 | validation Loss: 0.69595 | Training Acc: 61.333 | validation Acc: 50.303\n",
            "Epoch 003: | Training Loss: 0.63084 | validation Loss: 0.68335 | Training Acc: 65.167 | validation Acc: 51.513\n",
            "Epoch 004: | Training Loss: 0.61520 | validation Loss: 0.66246 | Training Acc: 68.000 | validation Acc: 65.053\n",
            "Epoch 005: | Training Loss: 0.60043 | validation Loss: 0.63378 | Training Acc: 71.000 | validation Acc: 68.986\n",
            "Epoch 006: | Training Loss: 0.58892 | validation Loss: 0.60410 | Training Acc: 72.500 | validation Acc: 72.466\n",
            "Epoch 007: | Training Loss: 0.57731 | validation Loss: 0.57998 | Training Acc: 73.500 | validation Acc: 74.054\n",
            "Epoch 008: | Training Loss: 0.56406 | validation Loss: 0.56216 | Training Acc: 76.000 | validation Acc: 75.265\n",
            "Epoch 009: | Training Loss: 0.55129 | validation Loss: 0.54922 | Training Acc: 76.833 | validation Acc: 75.643\n",
            "Epoch 010: | Training Loss: 0.54822 | validation Loss: 0.54067 | Training Acc: 76.500 | validation Acc: 76.324\n",
            "Epoch 011: | Training Loss: 0.53833 | validation Loss: 0.53232 | Training Acc: 77.000 | validation Acc: 76.475\n",
            "Epoch 012: | Training Loss: 0.52501 | validation Loss: 0.52434 | Training Acc: 78.000 | validation Acc: 76.778\n",
            "Epoch 013: | Training Loss: 0.52106 | validation Loss: 0.51800 | Training Acc: 78.500 | validation Acc: 76.853\n",
            "Epoch 014: | Training Loss: 0.51712 | validation Loss: 0.51448 | Training Acc: 79.000 | validation Acc: 77.080\n",
            "Epoch 015: | Training Loss: 0.51072 | validation Loss: 0.51337 | Training Acc: 78.667 | validation Acc: 77.231\n",
            "Epoch 016: | Training Loss: 0.50961 | validation Loss: 0.51152 | Training Acc: 78.667 | validation Acc: 77.610\n",
            "Epoch 017: | Training Loss: 0.50659 | validation Loss: 0.50983 | Training Acc: 78.333 | validation Acc: 77.231\n",
            "Epoch 018: | Training Loss: 0.50469 | validation Loss: 0.50781 | Training Acc: 78.833 | validation Acc: 76.929\n",
            "Epoch 019: | Training Loss: 0.50324 | validation Loss: 0.50600 | Training Acc: 78.500 | validation Acc: 76.626\n",
            "Epoch 020: | Training Loss: 0.49786 | validation Loss: 0.50420 | Training Acc: 79.167 | validation Acc: 76.929\n",
            "Epoch 021: | Training Loss: 0.48779 | validation Loss: 0.50198 | Training Acc: 79.500 | validation Acc: 77.458\n",
            "Epoch 022: | Training Loss: 0.49383 | validation Loss: 0.49965 | Training Acc: 79.500 | validation Acc: 77.383\n",
            "Epoch 023: | Training Loss: 0.48538 | validation Loss: 0.49868 | Training Acc: 79.833 | validation Acc: 77.156\n",
            "Epoch 024: | Training Loss: 0.48964 | validation Loss: 0.49785 | Training Acc: 79.500 | validation Acc: 77.383\n",
            "Epoch 025: | Training Loss: 0.49712 | validation Loss: 0.49654 | Training Acc: 78.833 | validation Acc: 77.458\n",
            "Epoch 026: | Training Loss: 0.49558 | validation Loss: 0.49570 | Training Acc: 78.833 | validation Acc: 77.610\n",
            "Epoch 027: | Training Loss: 0.48731 | validation Loss: 0.49513 | Training Acc: 79.500 | validation Acc: 77.458\n",
            "Epoch 028: | Training Loss: 0.47976 | validation Loss: 0.49559 | Training Acc: 80.000 | validation Acc: 77.156\n",
            "Epoch 029: | Training Loss: 0.49258 | validation Loss: 0.49456 | Training Acc: 78.833 | validation Acc: 77.307\n",
            "Epoch 030: | Training Loss: 0.47409 | validation Loss: 0.49435 | Training Acc: 80.167 | validation Acc: 77.156\n",
            "Epoch 031: | Training Loss: 0.47649 | validation Loss: 0.49328 | Training Acc: 79.833 | validation Acc: 77.156\n",
            "Epoch 032: | Training Loss: 0.47590 | validation Loss: 0.49305 | Training Acc: 79.833 | validation Acc: 77.156\n",
            "Epoch 033: | Training Loss: 0.48872 | validation Loss: 0.49345 | Training Acc: 79.333 | validation Acc: 77.383\n",
            "Epoch 034: | Training Loss: 0.48412 | validation Loss: 0.49259 | Training Acc: 79.667 | validation Acc: 77.231\n",
            "Epoch 035: | Training Loss: 0.47992 | validation Loss: 0.49184 | Training Acc: 79.833 | validation Acc: 77.383\n",
            "Epoch 036: | Training Loss: 0.48192 | validation Loss: 0.49188 | Training Acc: 79.500 | validation Acc: 77.685\n",
            "Epoch 037: | Training Loss: 0.48615 | validation Loss: 0.49219 | Training Acc: 79.833 | validation Acc: 77.610\n",
            "Epoch 038: | Training Loss: 0.47412 | validation Loss: 0.49160 | Training Acc: 80.000 | validation Acc: 77.383\n",
            "Epoch 039: | Training Loss: 0.47853 | validation Loss: 0.49071 | Training Acc: 79.833 | validation Acc: 77.080\n",
            "Epoch 040: | Training Loss: 0.47388 | validation Loss: 0.49046 | Training Acc: 79.667 | validation Acc: 77.005\n",
            "Epoch 041: | Training Loss: 0.47236 | validation Loss: 0.48995 | Training Acc: 80.333 | validation Acc: 77.231\n",
            "Epoch 042: | Training Loss: 0.47721 | validation Loss: 0.48998 | Training Acc: 79.667 | validation Acc: 77.307\n",
            "Epoch 043: | Training Loss: 0.46912 | validation Loss: 0.49016 | Training Acc: 80.000 | validation Acc: 77.307\n",
            "Epoch 044: | Training Loss: 0.48370 | validation Loss: 0.48933 | Training Acc: 79.500 | validation Acc: 77.534\n",
            "Epoch 045: | Training Loss: 0.47112 | validation Loss: 0.48842 | Training Acc: 80.167 | validation Acc: 77.307\n",
            "Epoch 046: | Training Loss: 0.47781 | validation Loss: 0.48840 | Training Acc: 79.667 | validation Acc: 77.307\n",
            "Epoch 047: | Training Loss: 0.47340 | validation Loss: 0.48841 | Training Acc: 80.000 | validation Acc: 77.383\n",
            "Epoch 048: | Training Loss: 0.47560 | validation Loss: 0.48731 | Training Acc: 79.833 | validation Acc: 77.005\n",
            "Epoch 049: | Training Loss: 0.47243 | validation Loss: 0.48737 | Training Acc: 79.833 | validation Acc: 77.383\n",
            "Epoch 050: | Training Loss: 0.46820 | validation Loss: 0.48676 | Training Acc: 80.167 | validation Acc: 77.383\n",
            "Epoch 051: | Training Loss: 0.47101 | validation Loss: 0.48719 | Training Acc: 80.500 | validation Acc: 77.307\n",
            "Epoch 052: | Training Loss: 0.46721 | validation Loss: 0.48674 | Training Acc: 80.333 | validation Acc: 77.458\n",
            "Epoch 053: | Training Loss: 0.47452 | validation Loss: 0.48666 | Training Acc: 80.333 | validation Acc: 77.307\n",
            "Epoch 054: | Training Loss: 0.47341 | validation Loss: 0.48664 | Training Acc: 79.667 | validation Acc: 77.307\n",
            "Epoch 055: | Training Loss: 0.46840 | validation Loss: 0.48671 | Training Acc: 80.500 | validation Acc: 77.307\n",
            "Epoch 056: | Training Loss: 0.47520 | validation Loss: 0.48786 | Training Acc: 79.833 | validation Acc: 77.458\n",
            "Epoch 057: | Training Loss: 0.46764 | validation Loss: 0.48833 | Training Acc: 80.500 | validation Acc: 77.307\n",
            "Epoch 058: | Training Loss: 0.47403 | validation Loss: 0.48862 | Training Acc: 80.167 | validation Acc: 77.231\n",
            "Epoch 059: | Training Loss: 0.47947 | validation Loss: 0.48879 | Training Acc: 80.167 | validation Acc: 77.307\n",
            "Epoch 060: | Training Loss: 0.46657 | validation Loss: 0.48802 | Training Acc: 80.667 | validation Acc: 77.156\n",
            "Epoch 061: | Training Loss: 0.46568 | validation Loss: 0.48732 | Training Acc: 80.500 | validation Acc: 77.307\n",
            "Epoch 062: | Training Loss: 0.47223 | validation Loss: 0.48762 | Training Acc: 80.167 | validation Acc: 77.307\n",
            "Epoch 063: | Training Loss: 0.46278 | validation Loss: 0.48758 | Training Acc: 80.833 | validation Acc: 77.156\n",
            "Epoch 064: | Training Loss: 0.47161 | validation Loss: 0.48764 | Training Acc: 80.333 | validation Acc: 77.458\n",
            "Epoch 065: | Training Loss: 0.47609 | validation Loss: 0.48766 | Training Acc: 79.833 | validation Acc: 77.458\n",
            "Epoch 066: | Training Loss: 0.46163 | validation Loss: 0.48748 | Training Acc: 80.500 | validation Acc: 77.307\n",
            "Epoch 067: | Training Loss: 0.46861 | validation Loss: 0.48783 | Training Acc: 80.333 | validation Acc: 77.156\n",
            "Epoch 068: | Training Loss: 0.47688 | validation Loss: 0.48794 | Training Acc: 80.000 | validation Acc: 77.307\n",
            "Epoch 069: | Training Loss: 0.46419 | validation Loss: 0.48613 | Training Acc: 80.500 | validation Acc: 77.458\n",
            "Epoch 070: | Training Loss: 0.47111 | validation Loss: 0.48686 | Training Acc: 79.667 | validation Acc: 77.458\n",
            "Epoch 071: | Training Loss: 0.46890 | validation Loss: 0.48742 | Training Acc: 80.000 | validation Acc: 77.534\n",
            "Epoch 072: | Training Loss: 0.46478 | validation Loss: 0.48730 | Training Acc: 80.167 | validation Acc: 77.307\n",
            "Epoch 073: | Training Loss: 0.47358 | validation Loss: 0.48798 | Training Acc: 79.667 | validation Acc: 77.383\n",
            "Epoch 074: | Training Loss: 0.46664 | validation Loss: 0.48749 | Training Acc: 79.833 | validation Acc: 77.307\n",
            "Epoch 075: | Training Loss: 0.46410 | validation Loss: 0.48683 | Training Acc: 80.333 | validation Acc: 77.383\n",
            "Epoch 076: | Training Loss: 0.46672 | validation Loss: 0.48657 | Training Acc: 80.500 | validation Acc: 77.458\n",
            "Epoch 077: | Training Loss: 0.47610 | validation Loss: 0.48674 | Training Acc: 79.667 | validation Acc: 77.383\n",
            "Epoch 078: | Training Loss: 0.46741 | validation Loss: 0.48611 | Training Acc: 79.667 | validation Acc: 77.685\n",
            "Epoch 079: | Training Loss: 0.47154 | validation Loss: 0.48619 | Training Acc: 79.500 | validation Acc: 77.534\n",
            "Epoch 080: | Training Loss: 0.46813 | validation Loss: 0.48689 | Training Acc: 79.833 | validation Acc: 77.383\n",
            "Epoch 081: | Training Loss: 0.46862 | validation Loss: 0.48631 | Training Acc: 80.167 | validation Acc: 77.534\n",
            "Epoch 082: | Training Loss: 0.46665 | validation Loss: 0.48689 | Training Acc: 80.000 | validation Acc: 77.307\n",
            "Epoch 083: | Training Loss: 0.46481 | validation Loss: 0.48651 | Training Acc: 80.333 | validation Acc: 77.231\n",
            "Epoch 084: | Training Loss: 0.46936 | validation Loss: 0.48650 | Training Acc: 80.167 | validation Acc: 77.534\n",
            "Epoch 085: | Training Loss: 0.47639 | validation Loss: 0.48629 | Training Acc: 79.667 | validation Acc: 77.685\n",
            "Epoch 086: | Training Loss: 0.46586 | validation Loss: 0.48642 | Training Acc: 80.333 | validation Acc: 77.761\n",
            "Epoch 087: | Training Loss: 0.46536 | validation Loss: 0.48548 | Training Acc: 80.167 | validation Acc: 77.610\n",
            "Epoch 088: | Training Loss: 0.47043 | validation Loss: 0.48574 | Training Acc: 80.333 | validation Acc: 77.534\n",
            "Epoch 089: | Training Loss: 0.46205 | validation Loss: 0.48603 | Training Acc: 80.667 | validation Acc: 77.231\n",
            "Epoch 090: | Training Loss: 0.46709 | validation Loss: 0.48569 | Training Acc: 80.167 | validation Acc: 77.231\n",
            "Epoch 091: | Training Loss: 0.46738 | validation Loss: 0.48544 | Training Acc: 80.167 | validation Acc: 77.383\n",
            "Epoch 092: | Training Loss: 0.46764 | validation Loss: 0.48453 | Training Acc: 80.500 | validation Acc: 77.534\n",
            "Epoch 093: | Training Loss: 0.46907 | validation Loss: 0.48466 | Training Acc: 80.167 | validation Acc: 77.912\n",
            "Epoch 094: | Training Loss: 0.46119 | validation Loss: 0.48495 | Training Acc: 80.833 | validation Acc: 77.610\n",
            "Epoch 095: | Training Loss: 0.47016 | validation Loss: 0.48538 | Training Acc: 80.167 | validation Acc: 77.610\n",
            "Epoch 096: | Training Loss: 0.46684 | validation Loss: 0.48543 | Training Acc: 80.500 | validation Acc: 77.534\n",
            "Epoch 097: | Training Loss: 0.46862 | validation Loss: 0.48538 | Training Acc: 79.833 | validation Acc: 77.610\n",
            "Epoch 098: | Training Loss: 0.46507 | validation Loss: 0.48543 | Training Acc: 80.167 | validation Acc: 77.685\n",
            "Epoch 099: | Training Loss: 0.46383 | validation Loss: 0.48646 | Training Acc: 80.500 | validation Acc: 77.231\n",
            "Epoch 100: | Training Loss: 0.46967 | validation Loss: 0.48572 | Training Acc: 80.500 | validation Acc: 77.534\n",
            "Epoch 101: | Training Loss: 0.45760 | validation Loss: 0.48537 | Training Acc: 80.667 | validation Acc: 77.307\n",
            "Epoch 102: | Training Loss: 0.46342 | validation Loss: 0.48560 | Training Acc: 80.333 | validation Acc: 77.458\n",
            "Epoch 103: | Training Loss: 0.46420 | validation Loss: 0.48611 | Training Acc: 80.500 | validation Acc: 77.534\n",
            "Epoch 104: | Training Loss: 0.47308 | validation Loss: 0.48506 | Training Acc: 79.833 | validation Acc: 77.383\n",
            "Epoch 105: | Training Loss: 0.46758 | validation Loss: 0.48488 | Training Acc: 79.667 | validation Acc: 77.685\n",
            "Epoch 106: | Training Loss: 0.46201 | validation Loss: 0.48481 | Training Acc: 81.000 | validation Acc: 77.534\n",
            "Epoch 107: | Training Loss: 0.46249 | validation Loss: 0.48533 | Training Acc: 80.333 | validation Acc: 77.383\n",
            "Epoch 108: | Training Loss: 0.47413 | validation Loss: 0.48604 | Training Acc: 79.500 | validation Acc: 77.156\n",
            "Epoch 109: | Training Loss: 0.46436 | validation Loss: 0.48557 | Training Acc: 80.667 | validation Acc: 77.080\n",
            "Epoch 110: | Training Loss: 0.45648 | validation Loss: 0.48595 | Training Acc: 80.833 | validation Acc: 77.080\n",
            "Epoch 111: | Training Loss: 0.46143 | validation Loss: 0.48672 | Training Acc: 80.500 | validation Acc: 77.080\n",
            "Epoch 112: | Training Loss: 0.45882 | validation Loss: 0.48674 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 113: | Training Loss: 0.46605 | validation Loss: 0.48719 | Training Acc: 80.000 | validation Acc: 77.383\n",
            "Epoch 114: | Training Loss: 0.46310 | validation Loss: 0.48678 | Training Acc: 80.500 | validation Acc: 77.761\n",
            "Epoch 115: | Training Loss: 0.46583 | validation Loss: 0.48677 | Training Acc: 80.500 | validation Acc: 77.383\n",
            "Epoch 116: | Training Loss: 0.46626 | validation Loss: 0.48719 | Training Acc: 80.500 | validation Acc: 77.458\n",
            "Epoch 117: | Training Loss: 0.45904 | validation Loss: 0.48809 | Training Acc: 80.500 | validation Acc: 77.231\n",
            "Epoch 118: | Training Loss: 0.46712 | validation Loss: 0.48834 | Training Acc: 80.167 | validation Acc: 77.458\n",
            "Epoch 119: | Training Loss: 0.45867 | validation Loss: 0.48685 | Training Acc: 80.333 | validation Acc: 77.307\n",
            "Epoch 120: | Training Loss: 0.46595 | validation Loss: 0.48675 | Training Acc: 80.500 | validation Acc: 77.458\n",
            "Epoch 121: | Training Loss: 0.45894 | validation Loss: 0.48694 | Training Acc: 80.500 | validation Acc: 77.383\n",
            "Epoch 122: | Training Loss: 0.45904 | validation Loss: 0.48631 | Training Acc: 81.000 | validation Acc: 77.383\n",
            "Epoch 123: | Training Loss: 0.46039 | validation Loss: 0.48651 | Training Acc: 80.667 | validation Acc: 77.231\n",
            "Epoch 124: | Training Loss: 0.46727 | validation Loss: 0.48689 | Training Acc: 80.500 | validation Acc: 77.231\n",
            "Epoch 125: | Training Loss: 0.45924 | validation Loss: 0.48780 | Training Acc: 80.833 | validation Acc: 77.080\n",
            "Epoch 126: | Training Loss: 0.46561 | validation Loss: 0.48769 | Training Acc: 80.167 | validation Acc: 77.080\n",
            "Epoch 127: | Training Loss: 0.46875 | validation Loss: 0.48754 | Training Acc: 79.500 | validation Acc: 77.156\n",
            "Epoch 128: | Training Loss: 0.46786 | validation Loss: 0.48724 | Training Acc: 80.500 | validation Acc: 76.853\n",
            "Epoch 129: | Training Loss: 0.46214 | validation Loss: 0.48752 | Training Acc: 80.333 | validation Acc: 77.307\n",
            "Epoch 130: | Training Loss: 0.45925 | validation Loss: 0.48693 | Training Acc: 80.500 | validation Acc: 77.231\n",
            "Epoch 131: | Training Loss: 0.46148 | validation Loss: 0.48654 | Training Acc: 80.333 | validation Acc: 77.156\n",
            "Epoch 132: | Training Loss: 0.46405 | validation Loss: 0.48517 | Training Acc: 79.833 | validation Acc: 77.383\n",
            "Epoch 133: | Training Loss: 0.46322 | validation Loss: 0.48488 | Training Acc: 80.500 | validation Acc: 77.231\n",
            "Epoch 134: | Training Loss: 0.46475 | validation Loss: 0.48519 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 135: | Training Loss: 0.45532 | validation Loss: 0.48567 | Training Acc: 80.500 | validation Acc: 77.231\n",
            "Epoch 136: | Training Loss: 0.46488 | validation Loss: 0.48650 | Training Acc: 80.500 | validation Acc: 77.307\n",
            "Epoch 137: | Training Loss: 0.45803 | validation Loss: 0.48651 | Training Acc: 80.833 | validation Acc: 76.929\n",
            "Epoch 138: | Training Loss: 0.45231 | validation Loss: 0.48663 | Training Acc: 80.833 | validation Acc: 77.231\n",
            "Epoch 139: | Training Loss: 0.45979 | validation Loss: 0.48680 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 140: | Training Loss: 0.45696 | validation Loss: 0.48707 | Training Acc: 80.333 | validation Acc: 77.231\n",
            "Epoch 141: | Training Loss: 0.45516 | validation Loss: 0.48639 | Training Acc: 81.000 | validation Acc: 77.231\n",
            "Epoch 142: | Training Loss: 0.46461 | validation Loss: 0.48581 | Training Acc: 80.167 | validation Acc: 77.383\n",
            "Epoch 143: | Training Loss: 0.45675 | validation Loss: 0.48648 | Training Acc: 80.500 | validation Acc: 77.307\n",
            "Epoch 144: | Training Loss: 0.47068 | validation Loss: 0.48811 | Training Acc: 79.500 | validation Acc: 77.156\n",
            "Epoch 145: | Training Loss: 0.45758 | validation Loss: 0.48834 | Training Acc: 80.667 | validation Acc: 77.231\n",
            "Epoch 146: | Training Loss: 0.46170 | validation Loss: 0.48800 | Training Acc: 80.833 | validation Acc: 77.307\n",
            "Epoch 147: | Training Loss: 0.46072 | validation Loss: 0.48753 | Training Acc: 80.333 | validation Acc: 77.383\n",
            "Epoch 148: | Training Loss: 0.45866 | validation Loss: 0.48719 | Training Acc: 80.667 | validation Acc: 77.307\n",
            "Epoch 149: | Training Loss: 0.44686 | validation Loss: 0.48770 | Training Acc: 81.167 | validation Acc: 77.307\n",
            "Epoch 150: | Training Loss: 0.46264 | validation Loss: 0.48752 | Training Acc: 80.333 | validation Acc: 77.156\n",
            "Epoch 151: | Training Loss: 0.46354 | validation Loss: 0.48618 | Training Acc: 80.333 | validation Acc: 77.080\n",
            "Epoch 152: | Training Loss: 0.46636 | validation Loss: 0.48611 | Training Acc: 80.333 | validation Acc: 77.383\n",
            "Epoch 153: | Training Loss: 0.46028 | validation Loss: 0.48648 | Training Acc: 81.000 | validation Acc: 77.307\n",
            "Epoch 154: | Training Loss: 0.46210 | validation Loss: 0.48698 | Training Acc: 80.667 | validation Acc: 77.383\n",
            "Epoch 155: | Training Loss: 0.45446 | validation Loss: 0.48819 | Training Acc: 81.000 | validation Acc: 77.458\n",
            "Epoch 156: | Training Loss: 0.45816 | validation Loss: 0.49004 | Training Acc: 81.167 | validation Acc: 77.383\n",
            "Epoch 157: | Training Loss: 0.45871 | validation Loss: 0.49029 | Training Acc: 80.333 | validation Acc: 77.080\n",
            "Epoch 158: | Training Loss: 0.46324 | validation Loss: 0.48964 | Training Acc: 80.333 | validation Acc: 77.005\n",
            "Epoch 159: | Training Loss: 0.45883 | validation Loss: 0.48822 | Training Acc: 80.167 | validation Acc: 77.307\n",
            "Epoch 160: | Training Loss: 0.45162 | validation Loss: 0.48675 | Training Acc: 81.167 | validation Acc: 77.458\n",
            "Epoch 161: | Training Loss: 0.45766 | validation Loss: 0.48680 | Training Acc: 80.500 | validation Acc: 77.458\n",
            "Epoch 162: | Training Loss: 0.45880 | validation Loss: 0.48718 | Training Acc: 80.500 | validation Acc: 77.458\n",
            "Epoch 163: | Training Loss: 0.45547 | validation Loss: 0.48809 | Training Acc: 80.333 | validation Acc: 77.231\n",
            "Epoch 164: | Training Loss: 0.45683 | validation Loss: 0.48785 | Training Acc: 80.333 | validation Acc: 77.156\n",
            "Epoch 165: | Training Loss: 0.45406 | validation Loss: 0.48830 | Training Acc: 81.000 | validation Acc: 77.231\n",
            "Epoch 166: | Training Loss: 0.45652 | validation Loss: 0.48875 | Training Acc: 80.333 | validation Acc: 77.231\n",
            "Epoch 167: | Training Loss: 0.45985 | validation Loss: 0.48875 | Training Acc: 80.333 | validation Acc: 77.231\n",
            "Epoch 168: | Training Loss: 0.45823 | validation Loss: 0.48835 | Training Acc: 80.500 | validation Acc: 77.231\n",
            "Epoch 169: | Training Loss: 0.45478 | validation Loss: 0.48790 | Training Acc: 80.500 | validation Acc: 77.383\n",
            "Epoch 170: | Training Loss: 0.45579 | validation Loss: 0.48773 | Training Acc: 80.667 | validation Acc: 77.156\n",
            "Epoch 171: | Training Loss: 0.46037 | validation Loss: 0.48750 | Training Acc: 80.333 | validation Acc: 77.458\n",
            "Epoch 172: | Training Loss: 0.45910 | validation Loss: 0.48778 | Training Acc: 80.500 | validation Acc: 77.383\n",
            "Epoch 173: | Training Loss: 0.46375 | validation Loss: 0.48784 | Training Acc: 80.667 | validation Acc: 77.231\n",
            "Epoch 174: | Training Loss: 0.45334 | validation Loss: 0.48740 | Training Acc: 81.000 | validation Acc: 77.080\n",
            "Epoch 175: | Training Loss: 0.46466 | validation Loss: 0.48691 | Training Acc: 80.333 | validation Acc: 77.231\n",
            "Epoch 176: | Training Loss: 0.46048 | validation Loss: 0.48731 | Training Acc: 80.500 | validation Acc: 77.005\n",
            "Epoch 177: | Training Loss: 0.46606 | validation Loss: 0.48784 | Training Acc: 80.667 | validation Acc: 77.156\n",
            "Epoch 178: | Training Loss: 0.45428 | validation Loss: 0.48825 | Training Acc: 80.833 | validation Acc: 77.156\n",
            "Epoch 179: | Training Loss: 0.44907 | validation Loss: 0.48845 | Training Acc: 80.167 | validation Acc: 77.383\n",
            "Epoch 180: | Training Loss: 0.45421 | validation Loss: 0.48865 | Training Acc: 80.833 | validation Acc: 77.383\n",
            "Epoch 181: | Training Loss: 0.45101 | validation Loss: 0.48937 | Training Acc: 81.167 | validation Acc: 77.458\n",
            "Epoch 182: | Training Loss: 0.45778 | validation Loss: 0.49011 | Training Acc: 80.333 | validation Acc: 77.231\n",
            "Epoch 183: | Training Loss: 0.45686 | validation Loss: 0.49047 | Training Acc: 80.667 | validation Acc: 77.156\n",
            "Epoch 184: | Training Loss: 0.45221 | validation Loss: 0.48975 | Training Acc: 81.000 | validation Acc: 77.080\n",
            "Epoch 185: | Training Loss: 0.45599 | validation Loss: 0.49094 | Training Acc: 80.833 | validation Acc: 76.778\n",
            "Epoch 186: | Training Loss: 0.46381 | validation Loss: 0.48924 | Training Acc: 80.333 | validation Acc: 76.929\n",
            "Epoch 187: | Training Loss: 0.46524 | validation Loss: 0.48806 | Training Acc: 80.667 | validation Acc: 77.458\n",
            "Epoch 188: | Training Loss: 0.45965 | validation Loss: 0.48918 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 189: | Training Loss: 0.45444 | validation Loss: 0.48934 | Training Acc: 80.500 | validation Acc: 77.307\n",
            "Epoch 190: | Training Loss: 0.45466 | validation Loss: 0.48985 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 191: | Training Loss: 0.45493 | validation Loss: 0.48903 | Training Acc: 80.833 | validation Acc: 77.231\n",
            "Epoch 192: | Training Loss: 0.46523 | validation Loss: 0.48907 | Training Acc: 80.333 | validation Acc: 77.307\n",
            "Epoch 193: | Training Loss: 0.46231 | validation Loss: 0.48908 | Training Acc: 80.333 | validation Acc: 77.231\n",
            "Epoch 194: | Training Loss: 0.45698 | validation Loss: 0.48886 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 195: | Training Loss: 0.45015 | validation Loss: 0.48922 | Training Acc: 80.833 | validation Acc: 77.080\n",
            "Epoch 196: | Training Loss: 0.44595 | validation Loss: 0.48867 | Training Acc: 81.167 | validation Acc: 77.458\n",
            "Epoch 197: | Training Loss: 0.44704 | validation Loss: 0.48861 | Training Acc: 81.167 | validation Acc: 77.383\n",
            "Epoch 198: | Training Loss: 0.45055 | validation Loss: 0.48809 | Training Acc: 80.667 | validation Acc: 77.231\n",
            "Epoch 199: | Training Loss: 0.45789 | validation Loss: 0.48792 | Training Acc: 80.333 | validation Acc: 77.458\n",
            "Epoch 200: | Training Loss: 0.45620 | validation Loss: 0.48859 | Training Acc: 80.833 | validation Acc: 77.307\n",
            "Epoch 201: | Training Loss: 0.45415 | validation Loss: 0.48929 | Training Acc: 81.000 | validation Acc: 77.231\n",
            "Epoch 202: | Training Loss: 0.45646 | validation Loss: 0.48959 | Training Acc: 80.833 | validation Acc: 77.383\n",
            "Epoch 203: | Training Loss: 0.45334 | validation Loss: 0.48948 | Training Acc: 80.833 | validation Acc: 77.534\n",
            "Epoch 204: | Training Loss: 0.45771 | validation Loss: 0.49012 | Training Acc: 80.833 | validation Acc: 77.534\n",
            "Epoch 205: | Training Loss: 0.45753 | validation Loss: 0.49059 | Training Acc: 80.333 | validation Acc: 77.383\n",
            "Epoch 206: | Training Loss: 0.45138 | validation Loss: 0.49089 | Training Acc: 80.667 | validation Acc: 77.156\n",
            "Epoch 207: | Training Loss: 0.45327 | validation Loss: 0.49183 | Training Acc: 80.333 | validation Acc: 76.626\n",
            "Epoch 208: | Training Loss: 0.45330 | validation Loss: 0.48941 | Training Acc: 80.667 | validation Acc: 77.080\n",
            "Epoch 209: | Training Loss: 0.46090 | validation Loss: 0.48864 | Training Acc: 80.333 | validation Acc: 77.080\n",
            "Epoch 210: | Training Loss: 0.45880 | validation Loss: 0.48950 | Training Acc: 80.667 | validation Acc: 77.080\n",
            "Epoch 211: | Training Loss: 0.45318 | validation Loss: 0.49018 | Training Acc: 80.667 | validation Acc: 77.080\n",
            "Epoch 212: | Training Loss: 0.45975 | validation Loss: 0.49039 | Training Acc: 80.167 | validation Acc: 77.231\n",
            "Epoch 213: | Training Loss: 0.44907 | validation Loss: 0.49003 | Training Acc: 80.667 | validation Acc: 77.080\n",
            "Epoch 214: | Training Loss: 0.45700 | validation Loss: 0.49156 | Training Acc: 80.333 | validation Acc: 77.005\n",
            "Epoch 215: | Training Loss: 0.45467 | validation Loss: 0.49158 | Training Acc: 81.000 | validation Acc: 77.156\n",
            "Epoch 216: | Training Loss: 0.45906 | validation Loss: 0.49236 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 217: | Training Loss: 0.45627 | validation Loss: 0.49289 | Training Acc: 80.000 | validation Acc: 77.307\n",
            "Epoch 218: | Training Loss: 0.45526 | validation Loss: 0.49362 | Training Acc: 80.833 | validation Acc: 77.005\n",
            "Epoch 219: | Training Loss: 0.45509 | validation Loss: 0.49292 | Training Acc: 80.833 | validation Acc: 77.005\n",
            "Epoch 220: | Training Loss: 0.46192 | validation Loss: 0.49236 | Training Acc: 79.667 | validation Acc: 77.156\n",
            "Epoch 221: | Training Loss: 0.45852 | validation Loss: 0.49210 | Training Acc: 80.500 | validation Acc: 76.853\n",
            "Epoch 222: | Training Loss: 0.45208 | validation Loss: 0.49300 | Training Acc: 81.000 | validation Acc: 76.929\n",
            "Epoch 223: | Training Loss: 0.45366 | validation Loss: 0.49262 | Training Acc: 80.667 | validation Acc: 76.853\n",
            "Epoch 224: | Training Loss: 0.45989 | validation Loss: 0.49342 | Training Acc: 80.333 | validation Acc: 77.156\n",
            "Epoch 225: | Training Loss: 0.45452 | validation Loss: 0.49288 | Training Acc: 80.833 | validation Acc: 77.231\n",
            "Epoch 226: | Training Loss: 0.44659 | validation Loss: 0.49291 | Training Acc: 81.167 | validation Acc: 77.685\n",
            "Epoch 227: | Training Loss: 0.46261 | validation Loss: 0.49452 | Training Acc: 80.333 | validation Acc: 77.383\n",
            "Epoch 228: | Training Loss: 0.45098 | validation Loss: 0.49550 | Training Acc: 80.667 | validation Acc: 77.156\n",
            "Epoch 229: | Training Loss: 0.45832 | validation Loss: 0.49400 | Training Acc: 80.000 | validation Acc: 76.929\n",
            "Epoch 230: | Training Loss: 0.44465 | validation Loss: 0.49127 | Training Acc: 81.333 | validation Acc: 77.156\n",
            "Epoch 231: | Training Loss: 0.45669 | validation Loss: 0.49054 | Training Acc: 80.333 | validation Acc: 77.307\n",
            "Epoch 232: | Training Loss: 0.45039 | validation Loss: 0.49011 | Training Acc: 81.167 | validation Acc: 77.534\n",
            "Epoch 233: | Training Loss: 0.46164 | validation Loss: 0.49035 | Training Acc: 80.000 | validation Acc: 77.458\n",
            "Epoch 234: | Training Loss: 0.45271 | validation Loss: 0.49251 | Training Acc: 81.000 | validation Acc: 77.231\n",
            "Epoch 235: | Training Loss: 0.45277 | validation Loss: 0.49384 | Training Acc: 81.167 | validation Acc: 77.307\n",
            "Epoch 236: | Training Loss: 0.45326 | validation Loss: 0.49371 | Training Acc: 80.500 | validation Acc: 77.231\n",
            "Epoch 237: | Training Loss: 0.45526 | validation Loss: 0.49387 | Training Acc: 80.833 | validation Acc: 77.231\n",
            "Epoch 238: | Training Loss: 0.45947 | validation Loss: 0.49220 | Training Acc: 80.167 | validation Acc: 77.231\n",
            "Epoch 239: | Training Loss: 0.44936 | validation Loss: 0.49130 | Training Acc: 80.833 | validation Acc: 77.383\n",
            "Epoch 240: | Training Loss: 0.45582 | validation Loss: 0.49045 | Training Acc: 80.667 | validation Acc: 77.383\n",
            "Epoch 241: | Training Loss: 0.45235 | validation Loss: 0.49014 | Training Acc: 81.167 | validation Acc: 77.231\n",
            "Epoch 242: | Training Loss: 0.44854 | validation Loss: 0.49068 | Training Acc: 80.667 | validation Acc: 77.080\n",
            "Epoch 243: | Training Loss: 0.45528 | validation Loss: 0.49000 | Training Acc: 80.667 | validation Acc: 77.307\n",
            "Epoch 244: | Training Loss: 0.45133 | validation Loss: 0.49081 | Training Acc: 80.500 | validation Acc: 77.080\n",
            "Epoch 245: | Training Loss: 0.45179 | validation Loss: 0.49107 | Training Acc: 81.167 | validation Acc: 77.080\n",
            "Epoch 246: | Training Loss: 0.45223 | validation Loss: 0.49241 | Training Acc: 80.667 | validation Acc: 77.231\n",
            "Epoch 247: | Training Loss: 0.45732 | validation Loss: 0.49230 | Training Acc: 80.333 | validation Acc: 77.231\n",
            "Epoch 248: | Training Loss: 0.45504 | validation Loss: 0.49236 | Training Acc: 80.333 | validation Acc: 77.156\n",
            "Epoch 249: | Training Loss: 0.45786 | validation Loss: 0.49194 | Training Acc: 80.333 | validation Acc: 77.156\n",
            "Epoch 250: | Training Loss: 0.44622 | validation Loss: 0.49164 | Training Acc: 80.833 | validation Acc: 77.231\n",
            "Epoch 251: | Training Loss: 0.45143 | validation Loss: 0.49218 | Training Acc: 80.833 | validation Acc: 77.156\n",
            "Epoch 252: | Training Loss: 0.44607 | validation Loss: 0.49368 | Training Acc: 80.833 | validation Acc: 77.080\n",
            "Epoch 253: | Training Loss: 0.45809 | validation Loss: 0.49476 | Training Acc: 80.500 | validation Acc: 77.231\n",
            "Epoch 254: | Training Loss: 0.46080 | validation Loss: 0.49461 | Training Acc: 80.500 | validation Acc: 77.383\n",
            "Epoch 255: | Training Loss: 0.45226 | validation Loss: 0.49410 | Training Acc: 81.333 | validation Acc: 77.231\n",
            "Epoch 256: | Training Loss: 0.45241 | validation Loss: 0.49281 | Training Acc: 81.000 | validation Acc: 77.307\n",
            "Epoch 257: | Training Loss: 0.45979 | validation Loss: 0.49232 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 258: | Training Loss: 0.45110 | validation Loss: 0.49061 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 259: | Training Loss: 0.45202 | validation Loss: 0.49145 | Training Acc: 80.833 | validation Acc: 77.156\n",
            "Epoch 260: | Training Loss: 0.45149 | validation Loss: 0.49369 | Training Acc: 80.833 | validation Acc: 77.156\n",
            "Epoch 261: | Training Loss: 0.45025 | validation Loss: 0.49451 | Training Acc: 80.667 | validation Acc: 77.080\n",
            "Epoch 262: | Training Loss: 0.45122 | validation Loss: 0.49373 | Training Acc: 80.833 | validation Acc: 76.778\n",
            "Epoch 263: | Training Loss: 0.44653 | validation Loss: 0.49231 | Training Acc: 81.000 | validation Acc: 76.853\n",
            "Epoch 264: | Training Loss: 0.45376 | validation Loss: 0.49278 | Training Acc: 80.667 | validation Acc: 76.626\n",
            "Epoch 265: | Training Loss: 0.45645 | validation Loss: 0.49409 | Training Acc: 80.500 | validation Acc: 76.778\n",
            "Epoch 266: | Training Loss: 0.44641 | validation Loss: 0.49344 | Training Acc: 81.000 | validation Acc: 76.702\n",
            "Epoch 267: | Training Loss: 0.44912 | validation Loss: 0.49432 | Training Acc: 81.000 | validation Acc: 76.778\n",
            "Epoch 268: | Training Loss: 0.44759 | validation Loss: 0.49408 | Training Acc: 81.167 | validation Acc: 77.080\n",
            "Epoch 269: | Training Loss: 0.44963 | validation Loss: 0.49484 | Training Acc: 80.667 | validation Acc: 77.080\n",
            "Epoch 270: | Training Loss: 0.45492 | validation Loss: 0.49383 | Training Acc: 80.833 | validation Acc: 77.080\n",
            "Epoch 271: | Training Loss: 0.45037 | validation Loss: 0.49369 | Training Acc: 80.833 | validation Acc: 76.778\n",
            "Epoch 272: | Training Loss: 0.44912 | validation Loss: 0.49300 | Training Acc: 80.667 | validation Acc: 76.702\n",
            "Epoch 273: | Training Loss: 0.44721 | validation Loss: 0.49471 | Training Acc: 81.667 | validation Acc: 77.156\n",
            "Epoch 274: | Training Loss: 0.44879 | validation Loss: 0.49543 | Training Acc: 81.167 | validation Acc: 77.005\n",
            "Epoch 275: | Training Loss: 0.44722 | validation Loss: 0.49411 | Training Acc: 80.833 | validation Acc: 77.005\n",
            "Epoch 276: | Training Loss: 0.46171 | validation Loss: 0.49400 | Training Acc: 80.167 | validation Acc: 77.156\n",
            "Epoch 277: | Training Loss: 0.44929 | validation Loss: 0.49444 | Training Acc: 81.000 | validation Acc: 77.005\n",
            "Epoch 278: | Training Loss: 0.44740 | validation Loss: 0.49389 | Training Acc: 80.833 | validation Acc: 77.080\n",
            "Epoch 279: | Training Loss: 0.44466 | validation Loss: 0.49297 | Training Acc: 80.833 | validation Acc: 77.005\n",
            "Epoch 280: | Training Loss: 0.44583 | validation Loss: 0.49384 | Training Acc: 81.167 | validation Acc: 77.231\n",
            "Epoch 281: | Training Loss: 0.44807 | validation Loss: 0.49475 | Training Acc: 81.000 | validation Acc: 76.929\n",
            "Epoch 282: | Training Loss: 0.45560 | validation Loss: 0.49412 | Training Acc: 80.500 | validation Acc: 76.853\n",
            "Epoch 283: | Training Loss: 0.45123 | validation Loss: 0.49408 | Training Acc: 80.500 | validation Acc: 76.853\n",
            "Epoch 284: | Training Loss: 0.44660 | validation Loss: 0.49403 | Training Acc: 80.833 | validation Acc: 76.929\n",
            "Epoch 285: | Training Loss: 0.44981 | validation Loss: 0.49403 | Training Acc: 81.167 | validation Acc: 77.005\n",
            "Epoch 286: | Training Loss: 0.45578 | validation Loss: 0.49592 | Training Acc: 80.833 | validation Acc: 77.231\n",
            "Epoch 287: | Training Loss: 0.44297 | validation Loss: 0.49628 | Training Acc: 81.167 | validation Acc: 77.231\n",
            "Epoch 288: | Training Loss: 0.44790 | validation Loss: 0.49597 | Training Acc: 80.833 | validation Acc: 77.080\n",
            "Epoch 289: | Training Loss: 0.45895 | validation Loss: 0.49514 | Training Acc: 80.000 | validation Acc: 77.156\n",
            "Epoch 290: | Training Loss: 0.44839 | validation Loss: 0.49417 | Training Acc: 80.833 | validation Acc: 77.383\n",
            "Epoch 291: | Training Loss: 0.44947 | validation Loss: 0.49365 | Training Acc: 80.833 | validation Acc: 77.231\n",
            "Epoch 292: | Training Loss: 0.45214 | validation Loss: 0.49254 | Training Acc: 80.667 | validation Acc: 77.156\n",
            "Epoch 293: | Training Loss: 0.44774 | validation Loss: 0.49412 | Training Acc: 80.833 | validation Acc: 77.156\n",
            "Epoch 294: | Training Loss: 0.44921 | validation Loss: 0.49408 | Training Acc: 80.833 | validation Acc: 76.929\n",
            "Epoch 295: | Training Loss: 0.44432 | validation Loss: 0.49456 | Training Acc: 81.500 | validation Acc: 76.929\n",
            "Epoch 296: | Training Loss: 0.45029 | validation Loss: 0.49362 | Training Acc: 81.000 | validation Acc: 76.853\n",
            "Epoch 297: | Training Loss: 0.44942 | validation Loss: 0.49221 | Training Acc: 80.833 | validation Acc: 76.702\n",
            "Epoch 298: | Training Loss: 0.44309 | validation Loss: 0.49147 | Training Acc: 81.333 | validation Acc: 77.458\n",
            "Epoch 299: | Training Loss: 0.44396 | validation Loss: 0.49180 | Training Acc: 81.167 | validation Acc: 77.307\n",
            "Epoch 300: | Training Loss: 0.45729 | validation Loss: 0.49258 | Training Acc: 80.333 | validation Acc: 77.080\n",
            "Epoch 301: | Training Loss: 0.44989 | validation Loss: 0.49249 | Training Acc: 81.000 | validation Acc: 77.156\n",
            "Epoch 302: | Training Loss: 0.45195 | validation Loss: 0.49333 | Training Acc: 80.500 | validation Acc: 77.080\n",
            "Epoch 303: | Training Loss: 0.44814 | validation Loss: 0.49395 | Training Acc: 80.667 | validation Acc: 77.307\n",
            "Epoch 304: | Training Loss: 0.45021 | validation Loss: 0.49401 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 305: | Training Loss: 0.45086 | validation Loss: 0.49428 | Training Acc: 80.833 | validation Acc: 77.080\n",
            "Epoch 306: | Training Loss: 0.45512 | validation Loss: 0.49309 | Training Acc: 80.333 | validation Acc: 76.929\n",
            "Epoch 307: | Training Loss: 0.45665 | validation Loss: 0.49272 | Training Acc: 80.167 | validation Acc: 77.156\n",
            "Epoch 308: | Training Loss: 0.45366 | validation Loss: 0.49248 | Training Acc: 81.000 | validation Acc: 76.929\n",
            "Epoch 309: | Training Loss: 0.44661 | validation Loss: 0.49350 | Training Acc: 81.000 | validation Acc: 77.231\n",
            "Epoch 310: | Training Loss: 0.44590 | validation Loss: 0.49386 | Training Acc: 80.833 | validation Acc: 77.307\n",
            "Epoch 311: | Training Loss: 0.45157 | validation Loss: 0.49450 | Training Acc: 80.167 | validation Acc: 77.307\n",
            "Epoch 312: | Training Loss: 0.45499 | validation Loss: 0.49341 | Training Acc: 80.667 | validation Acc: 77.005\n",
            "Epoch 313: | Training Loss: 0.44791 | validation Loss: 0.49335 | Training Acc: 80.667 | validation Acc: 76.778\n",
            "Epoch 314: | Training Loss: 0.44728 | validation Loss: 0.49355 | Training Acc: 81.000 | validation Acc: 76.929\n",
            "Epoch 315: | Training Loss: 0.44319 | validation Loss: 0.49513 | Training Acc: 81.500 | validation Acc: 77.005\n",
            "Epoch 316: | Training Loss: 0.44617 | validation Loss: 0.49555 | Training Acc: 80.667 | validation Acc: 76.929\n",
            "Epoch 317: | Training Loss: 0.44139 | validation Loss: 0.49479 | Training Acc: 81.500 | validation Acc: 76.778\n",
            "Epoch 318: | Training Loss: 0.45253 | validation Loss: 0.49511 | Training Acc: 80.500 | validation Acc: 76.929\n",
            "Epoch 319: | Training Loss: 0.44735 | validation Loss: 0.49356 | Training Acc: 81.167 | validation Acc: 76.929\n",
            "Epoch 320: | Training Loss: 0.43673 | validation Loss: 0.49381 | Training Acc: 81.167 | validation Acc: 77.156\n",
            "Epoch 321: | Training Loss: 0.44651 | validation Loss: 0.49382 | Training Acc: 81.333 | validation Acc: 77.080\n",
            "Epoch 322: | Training Loss: 0.44922 | validation Loss: 0.49569 | Training Acc: 81.000 | validation Acc: 77.005\n",
            "Epoch 323: | Training Loss: 0.44828 | validation Loss: 0.49771 | Training Acc: 80.667 | validation Acc: 76.929\n",
            "Epoch 324: | Training Loss: 0.44811 | validation Loss: 0.49649 | Training Acc: 81.000 | validation Acc: 76.853\n",
            "Epoch 325: | Training Loss: 0.45403 | validation Loss: 0.49526 | Training Acc: 80.333 | validation Acc: 77.005\n",
            "Epoch 326: | Training Loss: 0.44597 | validation Loss: 0.49487 | Training Acc: 81.333 | validation Acc: 77.231\n",
            "Epoch 327: | Training Loss: 0.44851 | validation Loss: 0.49491 | Training Acc: 81.333 | validation Acc: 77.080\n",
            "Epoch 328: | Training Loss: 0.44025 | validation Loss: 0.49534 | Training Acc: 80.667 | validation Acc: 77.307\n",
            "Epoch 329: | Training Loss: 0.44626 | validation Loss: 0.49508 | Training Acc: 80.667 | validation Acc: 77.231\n",
            "Epoch 330: | Training Loss: 0.44704 | validation Loss: 0.49499 | Training Acc: 81.000 | validation Acc: 77.458\n",
            "Epoch 331: | Training Loss: 0.45416 | validation Loss: 0.49518 | Training Acc: 80.333 | validation Acc: 77.383\n",
            "Epoch 332: | Training Loss: 0.44305 | validation Loss: 0.49649 | Training Acc: 81.000 | validation Acc: 77.383\n",
            "Epoch 333: | Training Loss: 0.44003 | validation Loss: 0.49627 | Training Acc: 81.167 | validation Acc: 77.005\n",
            "Epoch 334: | Training Loss: 0.45030 | validation Loss: 0.49568 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 335: | Training Loss: 0.44822 | validation Loss: 0.49491 | Training Acc: 81.167 | validation Acc: 77.231\n",
            "Epoch 336: | Training Loss: 0.45377 | validation Loss: 0.49459 | Training Acc: 80.833 | validation Acc: 77.307\n",
            "Epoch 337: | Training Loss: 0.45075 | validation Loss: 0.49515 | Training Acc: 80.833 | validation Acc: 76.778\n",
            "Epoch 338: | Training Loss: 0.45456 | validation Loss: 0.49453 | Training Acc: 80.000 | validation Acc: 76.702\n",
            "Epoch 339: | Training Loss: 0.44001 | validation Loss: 0.49412 | Training Acc: 80.500 | validation Acc: 76.778\n",
            "Epoch 340: | Training Loss: 0.44975 | validation Loss: 0.49629 | Training Acc: 81.000 | validation Acc: 77.005\n",
            "Epoch 341: | Training Loss: 0.44046 | validation Loss: 0.49562 | Training Acc: 81.000 | validation Acc: 77.307\n",
            "Epoch 342: | Training Loss: 0.44272 | validation Loss: 0.49641 | Training Acc: 81.333 | validation Acc: 77.231\n",
            "Epoch 343: | Training Loss: 0.45143 | validation Loss: 0.49621 | Training Acc: 80.833 | validation Acc: 77.156\n",
            "Epoch 344: | Training Loss: 0.44165 | validation Loss: 0.49542 | Training Acc: 81.167 | validation Acc: 77.005\n",
            "Epoch 345: | Training Loss: 0.44646 | validation Loss: 0.49581 | Training Acc: 81.000 | validation Acc: 76.853\n",
            "Epoch 346: | Training Loss: 0.44731 | validation Loss: 0.49331 | Training Acc: 80.167 | validation Acc: 77.307\n",
            "Epoch 347: | Training Loss: 0.45416 | validation Loss: 0.49335 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 348: | Training Loss: 0.44760 | validation Loss: 0.49271 | Training Acc: 80.667 | validation Acc: 77.080\n",
            "Epoch 349: | Training Loss: 0.43857 | validation Loss: 0.49278 | Training Acc: 81.167 | validation Acc: 77.005\n",
            "Epoch 350: | Training Loss: 0.45297 | validation Loss: 0.49508 | Training Acc: 80.167 | validation Acc: 76.929\n",
            "Epoch 351: | Training Loss: 0.44652 | validation Loss: 0.49495 | Training Acc: 80.833 | validation Acc: 77.005\n",
            "Epoch 352: | Training Loss: 0.45844 | validation Loss: 0.49395 | Training Acc: 80.333 | validation Acc: 77.458\n",
            "Epoch 353: | Training Loss: 0.45289 | validation Loss: 0.49343 | Training Acc: 80.500 | validation Acc: 77.534\n",
            "Epoch 354: | Training Loss: 0.44591 | validation Loss: 0.49423 | Training Acc: 80.500 | validation Acc: 77.231\n",
            "Epoch 355: | Training Loss: 0.44926 | validation Loss: 0.49400 | Training Acc: 80.500 | validation Acc: 77.534\n",
            "Epoch 356: | Training Loss: 0.44373 | validation Loss: 0.49504 | Training Acc: 81.500 | validation Acc: 77.458\n",
            "Epoch 357: | Training Loss: 0.45538 | validation Loss: 0.49605 | Training Acc: 80.833 | validation Acc: 77.307\n",
            "Epoch 358: | Training Loss: 0.44683 | validation Loss: 0.49617 | Training Acc: 81.000 | validation Acc: 77.307\n",
            "Epoch 359: | Training Loss: 0.44315 | validation Loss: 0.49618 | Training Acc: 81.000 | validation Acc: 77.156\n",
            "Epoch 360: | Training Loss: 0.44149 | validation Loss: 0.49500 | Training Acc: 80.833 | validation Acc: 77.231\n",
            "Epoch 361: | Training Loss: 0.45409 | validation Loss: 0.49512 | Training Acc: 80.333 | validation Acc: 77.231\n",
            "Epoch 362: | Training Loss: 0.45258 | validation Loss: 0.49534 | Training Acc: 80.833 | validation Acc: 77.383\n",
            "Epoch 363: | Training Loss: 0.44705 | validation Loss: 0.49456 | Training Acc: 80.667 | validation Acc: 77.383\n",
            "Epoch 364: | Training Loss: 0.44127 | validation Loss: 0.49499 | Training Acc: 81.333 | validation Acc: 77.307\n",
            "Epoch 365: | Training Loss: 0.44637 | validation Loss: 0.49511 | Training Acc: 80.667 | validation Acc: 76.929\n",
            "Epoch 366: | Training Loss: 0.45199 | validation Loss: 0.49520 | Training Acc: 80.667 | validation Acc: 77.080\n",
            "Epoch 367: | Training Loss: 0.45081 | validation Loss: 0.49461 | Training Acc: 80.833 | validation Acc: 77.080\n",
            "Epoch 368: | Training Loss: 0.45407 | validation Loss: 0.49502 | Training Acc: 80.667 | validation Acc: 77.383\n",
            "Epoch 369: | Training Loss: 0.45656 | validation Loss: 0.49587 | Training Acc: 80.333 | validation Acc: 77.383\n",
            "Epoch 370: | Training Loss: 0.44364 | validation Loss: 0.49654 | Training Acc: 81.000 | validation Acc: 77.156\n",
            "Epoch 371: | Training Loss: 0.44665 | validation Loss: 0.49706 | Training Acc: 81.167 | validation Acc: 77.156\n",
            "Epoch 372: | Training Loss: 0.44861 | validation Loss: 0.49704 | Training Acc: 80.667 | validation Acc: 76.929\n",
            "Epoch 373: | Training Loss: 0.43800 | validation Loss: 0.49781 | Training Acc: 81.000 | validation Acc: 77.080\n",
            "Epoch 374: | Training Loss: 0.45185 | validation Loss: 0.49656 | Training Acc: 81.000 | validation Acc: 77.005\n",
            "Epoch 375: | Training Loss: 0.45092 | validation Loss: 0.49431 | Training Acc: 80.833 | validation Acc: 77.383\n",
            "Epoch 376: | Training Loss: 0.44189 | validation Loss: 0.49389 | Training Acc: 81.000 | validation Acc: 77.458\n",
            "Epoch 377: | Training Loss: 0.45552 | validation Loss: 0.49407 | Training Acc: 80.500 | validation Acc: 77.307\n",
            "Epoch 378: | Training Loss: 0.44539 | validation Loss: 0.49557 | Training Acc: 80.667 | validation Acc: 77.383\n",
            "Epoch 379: | Training Loss: 0.44432 | validation Loss: 0.49494 | Training Acc: 81.000 | validation Acc: 77.231\n",
            "Epoch 380: | Training Loss: 0.44972 | validation Loss: 0.49523 | Training Acc: 81.000 | validation Acc: 77.231\n",
            "Epoch 381: | Training Loss: 0.44208 | validation Loss: 0.49439 | Training Acc: 81.000 | validation Acc: 77.307\n",
            "Epoch 382: | Training Loss: 0.44071 | validation Loss: 0.49542 | Training Acc: 81.167 | validation Acc: 77.005\n",
            "Epoch 383: | Training Loss: 0.44234 | validation Loss: 0.49590 | Training Acc: 81.833 | validation Acc: 77.458\n",
            "Epoch 384: | Training Loss: 0.44646 | validation Loss: 0.49573 | Training Acc: 80.833 | validation Acc: 77.458\n",
            "Epoch 385: | Training Loss: 0.44284 | validation Loss: 0.49564 | Training Acc: 81.167 | validation Acc: 77.458\n",
            "Epoch 386: | Training Loss: 0.45136 | validation Loss: 0.49629 | Training Acc: 80.833 | validation Acc: 76.929\n",
            "Epoch 387: | Training Loss: 0.44614 | validation Loss: 0.49608 | Training Acc: 80.833 | validation Acc: 77.231\n",
            "Epoch 388: | Training Loss: 0.44460 | validation Loss: 0.49480 | Training Acc: 80.833 | validation Acc: 77.231\n",
            "Epoch 389: | Training Loss: 0.44832 | validation Loss: 0.49394 | Training Acc: 80.500 | validation Acc: 77.080\n",
            "Epoch 390: | Training Loss: 0.43980 | validation Loss: 0.49474 | Training Acc: 81.167 | validation Acc: 77.156\n",
            "Epoch 391: | Training Loss: 0.45106 | validation Loss: 0.49580 | Training Acc: 80.667 | validation Acc: 77.005\n",
            "Epoch 392: | Training Loss: 0.44271 | validation Loss: 0.49611 | Training Acc: 81.167 | validation Acc: 77.231\n",
            "Epoch 393: | Training Loss: 0.44615 | validation Loss: 0.49522 | Training Acc: 81.000 | validation Acc: 77.231\n",
            "Epoch 394: | Training Loss: 0.44455 | validation Loss: 0.49467 | Training Acc: 81.000 | validation Acc: 77.005\n",
            "Epoch 395: | Training Loss: 0.43677 | validation Loss: 0.49362 | Training Acc: 81.667 | validation Acc: 77.383\n",
            "Epoch 396: | Training Loss: 0.44433 | validation Loss: 0.49392 | Training Acc: 81.000 | validation Acc: 77.534\n",
            "Epoch 397: | Training Loss: 0.43803 | validation Loss: 0.49471 | Training Acc: 81.333 | validation Acc: 77.307\n",
            "Epoch 398: | Training Loss: 0.44699 | validation Loss: 0.49566 | Training Acc: 80.833 | validation Acc: 77.005\n",
            "Epoch 399: | Training Loss: 0.45810 | validation Loss: 0.49646 | Training Acc: 80.333 | validation Acc: 76.929\n",
            "Epoch 400: | Training Loss: 0.44861 | validation Loss: 0.49684 | Training Acc: 81.167 | validation Acc: 76.702\n",
            "Epoch 401: | Training Loss: 0.44600 | validation Loss: 0.49808 | Training Acc: 80.667 | validation Acc: 77.231\n",
            "Epoch 402: | Training Loss: 0.44118 | validation Loss: 0.49630 | Training Acc: 81.167 | validation Acc: 77.458\n",
            "Epoch 403: | Training Loss: 0.44465 | validation Loss: 0.49684 | Training Acc: 80.833 | validation Acc: 77.307\n",
            "Epoch 404: | Training Loss: 0.45210 | validation Loss: 0.49789 | Training Acc: 81.000 | validation Acc: 77.005\n",
            "Epoch 405: | Training Loss: 0.44100 | validation Loss: 0.49695 | Training Acc: 81.167 | validation Acc: 77.080\n",
            "Epoch 406: | Training Loss: 0.43746 | validation Loss: 0.49801 | Training Acc: 81.333 | validation Acc: 77.231\n",
            "Epoch 407: | Training Loss: 0.44818 | validation Loss: 0.49693 | Training Acc: 80.667 | validation Acc: 77.383\n",
            "Epoch 408: | Training Loss: 0.45026 | validation Loss: 0.49679 | Training Acc: 80.333 | validation Acc: 76.929\n",
            "Epoch 409: | Training Loss: 0.44330 | validation Loss: 0.49828 | Training Acc: 81.000 | validation Acc: 76.853\n",
            "Epoch 410: | Training Loss: 0.44398 | validation Loss: 0.49740 | Training Acc: 80.667 | validation Acc: 77.005\n",
            "Epoch 411: | Training Loss: 0.43864 | validation Loss: 0.49705 | Training Acc: 81.000 | validation Acc: 77.005\n",
            "Epoch 412: | Training Loss: 0.43869 | validation Loss: 0.49712 | Training Acc: 81.167 | validation Acc: 77.005\n",
            "Epoch 413: | Training Loss: 0.44093 | validation Loss: 0.49807 | Training Acc: 81.167 | validation Acc: 77.005\n",
            "Epoch 414: | Training Loss: 0.44851 | validation Loss: 0.49867 | Training Acc: 80.667 | validation Acc: 77.156\n",
            "Epoch 415: | Training Loss: 0.44422 | validation Loss: 0.49786 | Training Acc: 80.833 | validation Acc: 77.005\n",
            "Epoch 416: | Training Loss: 0.44520 | validation Loss: 0.49706 | Training Acc: 81.000 | validation Acc: 77.005\n",
            "Epoch 417: | Training Loss: 0.44686 | validation Loss: 0.49618 | Training Acc: 81.000 | validation Acc: 77.080\n",
            "Epoch 418: | Training Loss: 0.44207 | validation Loss: 0.49732 | Training Acc: 80.667 | validation Acc: 76.929\n",
            "Epoch 419: | Training Loss: 0.44649 | validation Loss: 0.49779 | Training Acc: 80.667 | validation Acc: 77.005\n",
            "Epoch 420: | Training Loss: 0.43729 | validation Loss: 0.49782 | Training Acc: 81.500 | validation Acc: 76.929\n",
            "Epoch 421: | Training Loss: 0.44449 | validation Loss: 0.49789 | Training Acc: 81.000 | validation Acc: 76.778\n",
            "Epoch 422: | Training Loss: 0.44074 | validation Loss: 0.49727 | Training Acc: 81.333 | validation Acc: 77.231\n",
            "Epoch 423: | Training Loss: 0.44375 | validation Loss: 0.49880 | Training Acc: 80.833 | validation Acc: 77.383\n",
            "Epoch 424: | Training Loss: 0.44741 | validation Loss: 0.50075 | Training Acc: 80.833 | validation Acc: 76.853\n",
            "Epoch 425: | Training Loss: 0.45182 | validation Loss: 0.49961 | Training Acc: 80.333 | validation Acc: 76.778\n",
            "Epoch 426: | Training Loss: 0.43725 | validation Loss: 0.49811 | Training Acc: 81.167 | validation Acc: 76.702\n",
            "Epoch 427: | Training Loss: 0.44380 | validation Loss: 0.49950 | Training Acc: 80.833 | validation Acc: 76.702\n",
            "Epoch 428: | Training Loss: 0.44834 | validation Loss: 0.49981 | Training Acc: 80.667 | validation Acc: 77.080\n",
            "Epoch 429: | Training Loss: 0.44818 | validation Loss: 0.49965 | Training Acc: 81.000 | validation Acc: 77.458\n",
            "Epoch 430: | Training Loss: 0.44469 | validation Loss: 0.49889 | Training Acc: 80.833 | validation Acc: 77.307\n",
            "Epoch 431: | Training Loss: 0.44220 | validation Loss: 0.49844 | Training Acc: 80.833 | validation Acc: 77.156\n",
            "Epoch 432: | Training Loss: 0.44450 | validation Loss: 0.50003 | Training Acc: 81.167 | validation Acc: 77.005\n",
            "Epoch 433: | Training Loss: 0.44908 | validation Loss: 0.50156 | Training Acc: 80.667 | validation Acc: 76.853\n",
            "Epoch 434: | Training Loss: 0.45601 | validation Loss: 0.49889 | Training Acc: 80.167 | validation Acc: 77.005\n",
            "Epoch 435: | Training Loss: 0.44217 | validation Loss: 0.49818 | Training Acc: 81.333 | validation Acc: 77.080\n",
            "Epoch 436: | Training Loss: 0.45037 | validation Loss: 0.49862 | Training Acc: 80.667 | validation Acc: 77.156\n",
            "Epoch 437: | Training Loss: 0.45125 | validation Loss: 0.49958 | Training Acc: 81.333 | validation Acc: 77.080\n",
            "Epoch 438: | Training Loss: 0.44574 | validation Loss: 0.49984 | Training Acc: 80.833 | validation Acc: 76.626\n",
            "Epoch 439: | Training Loss: 0.44255 | validation Loss: 0.49935 | Training Acc: 81.167 | validation Acc: 76.551\n",
            "Epoch 440: | Training Loss: 0.44035 | validation Loss: 0.49910 | Training Acc: 81.333 | validation Acc: 76.929\n",
            "Epoch 441: | Training Loss: 0.45490 | validation Loss: 0.49749 | Training Acc: 80.333 | validation Acc: 77.307\n",
            "Epoch 442: | Training Loss: 0.44239 | validation Loss: 0.49586 | Training Acc: 81.000 | validation Acc: 77.156\n",
            "Epoch 443: | Training Loss: 0.44434 | validation Loss: 0.49554 | Training Acc: 81.000 | validation Acc: 77.156\n",
            "Epoch 444: | Training Loss: 0.44066 | validation Loss: 0.49734 | Training Acc: 80.833 | validation Acc: 77.080\n",
            "Epoch 445: | Training Loss: 0.43304 | validation Loss: 0.49784 | Training Acc: 81.333 | validation Acc: 77.005\n",
            "Epoch 446: | Training Loss: 0.44876 | validation Loss: 0.49773 | Training Acc: 80.500 | validation Acc: 77.080\n",
            "Epoch 447: | Training Loss: 0.44565 | validation Loss: 0.49967 | Training Acc: 80.667 | validation Acc: 76.929\n",
            "Epoch 448: | Training Loss: 0.44008 | validation Loss: 0.49855 | Training Acc: 81.000 | validation Acc: 76.778\n",
            "Epoch 449: | Training Loss: 0.44257 | validation Loss: 0.49839 | Training Acc: 81.167 | validation Acc: 76.778\n",
            "Epoch 450: | Training Loss: 0.44966 | validation Loss: 0.49840 | Training Acc: 80.833 | validation Acc: 77.156\n",
            "Epoch 451: | Training Loss: 0.44312 | validation Loss: 0.49764 | Training Acc: 80.833 | validation Acc: 77.005\n",
            "Epoch 452: | Training Loss: 0.44983 | validation Loss: 0.49844 | Training Acc: 80.667 | validation Acc: 76.702\n",
            "Epoch 453: | Training Loss: 0.44265 | validation Loss: 0.49794 | Training Acc: 80.667 | validation Acc: 77.005\n",
            "Epoch 454: | Training Loss: 0.45342 | validation Loss: 0.49769 | Training Acc: 80.333 | validation Acc: 77.080\n",
            "Epoch 455: | Training Loss: 0.45544 | validation Loss: 0.49640 | Training Acc: 80.500 | validation Acc: 77.080\n",
            "Epoch 456: | Training Loss: 0.43468 | validation Loss: 0.49842 | Training Acc: 81.500 | validation Acc: 77.005\n",
            "Epoch 457: | Training Loss: 0.43909 | validation Loss: 0.49754 | Training Acc: 81.167 | validation Acc: 77.156\n",
            "Epoch 458: | Training Loss: 0.44350 | validation Loss: 0.49665 | Training Acc: 81.167 | validation Acc: 77.156\n",
            "Epoch 459: | Training Loss: 0.44627 | validation Loss: 0.49830 | Training Acc: 80.667 | validation Acc: 77.156\n",
            "Epoch 460: | Training Loss: 0.43517 | validation Loss: 0.49779 | Training Acc: 81.500 | validation Acc: 77.080\n",
            "Epoch 461: | Training Loss: 0.44681 | validation Loss: 0.49800 | Training Acc: 81.000 | validation Acc: 77.080\n",
            "Epoch 462: | Training Loss: 0.43867 | validation Loss: 0.49668 | Training Acc: 81.667 | validation Acc: 77.307\n",
            "Epoch 463: | Training Loss: 0.43794 | validation Loss: 0.49720 | Training Acc: 81.167 | validation Acc: 77.080\n",
            "Epoch 464: | Training Loss: 0.43986 | validation Loss: 0.49944 | Training Acc: 81.333 | validation Acc: 77.080\n",
            "Epoch 465: | Training Loss: 0.44336 | validation Loss: 0.50113 | Training Acc: 81.000 | validation Acc: 77.005\n",
            "Epoch 466: | Training Loss: 0.44028 | validation Loss: 0.50174 | Training Acc: 81.167 | validation Acc: 77.231\n",
            "Epoch 467: | Training Loss: 0.43982 | validation Loss: 0.50059 | Training Acc: 81.167 | validation Acc: 77.383\n",
            "Epoch 468: | Training Loss: 0.44309 | validation Loss: 0.50027 | Training Acc: 80.833 | validation Acc: 77.005\n",
            "Epoch 469: | Training Loss: 0.43677 | validation Loss: 0.49976 | Training Acc: 81.500 | validation Acc: 77.080\n",
            "Epoch 470: | Training Loss: 0.43727 | validation Loss: 0.50104 | Training Acc: 81.667 | validation Acc: 76.853\n",
            "Epoch 471: | Training Loss: 0.44175 | validation Loss: 0.50086 | Training Acc: 80.833 | validation Acc: 77.005\n",
            "Epoch 472: | Training Loss: 0.43663 | validation Loss: 0.50066 | Training Acc: 80.833 | validation Acc: 77.005\n",
            "Epoch 473: | Training Loss: 0.44311 | validation Loss: 0.50048 | Training Acc: 81.000 | validation Acc: 76.702\n",
            "Epoch 474: | Training Loss: 0.44007 | validation Loss: 0.50182 | Training Acc: 81.000 | validation Acc: 76.853\n",
            "Epoch 475: | Training Loss: 0.43656 | validation Loss: 0.50191 | Training Acc: 81.667 | validation Acc: 76.778\n",
            "Epoch 476: | Training Loss: 0.43871 | validation Loss: 0.50051 | Training Acc: 81.000 | validation Acc: 76.929\n",
            "Epoch 477: | Training Loss: 0.44320 | validation Loss: 0.49962 | Training Acc: 81.000 | validation Acc: 76.929\n",
            "Epoch 478: | Training Loss: 0.44894 | validation Loss: 0.49840 | Training Acc: 80.333 | validation Acc: 77.156\n",
            "Epoch 479: | Training Loss: 0.44799 | validation Loss: 0.49745 | Training Acc: 80.667 | validation Acc: 77.156\n",
            "Epoch 480: | Training Loss: 0.44628 | validation Loss: 0.49624 | Training Acc: 81.000 | validation Acc: 77.231\n",
            "Epoch 481: | Training Loss: 0.44306 | validation Loss: 0.49722 | Training Acc: 80.833 | validation Acc: 77.156\n",
            "Epoch 482: | Training Loss: 0.44290 | validation Loss: 0.49679 | Training Acc: 80.500 | validation Acc: 77.156\n",
            "Epoch 483: | Training Loss: 0.44787 | validation Loss: 0.49564 | Training Acc: 80.667 | validation Acc: 77.231\n",
            "Epoch 484: | Training Loss: 0.43626 | validation Loss: 0.49590 | Training Acc: 81.333 | validation Acc: 77.231\n",
            "Epoch 485: | Training Loss: 0.43751 | validation Loss: 0.49648 | Training Acc: 81.667 | validation Acc: 76.853\n",
            "Epoch 486: | Training Loss: 0.44634 | validation Loss: 0.49785 | Training Acc: 80.833 | validation Acc: 77.005\n",
            "Epoch 487: | Training Loss: 0.44087 | validation Loss: 0.49802 | Training Acc: 81.333 | validation Acc: 77.080\n",
            "Epoch 488: | Training Loss: 0.44685 | validation Loss: 0.49736 | Training Acc: 81.000 | validation Acc: 77.080\n",
            "Epoch 489: | Training Loss: 0.44620 | validation Loss: 0.49733 | Training Acc: 80.833 | validation Acc: 77.231\n",
            "Epoch 490: | Training Loss: 0.43897 | validation Loss: 0.49676 | Training Acc: 81.000 | validation Acc: 77.383\n",
            "Epoch 491: | Training Loss: 0.45174 | validation Loss: 0.49855 | Training Acc: 80.000 | validation Acc: 77.080\n",
            "Epoch 492: | Training Loss: 0.43545 | validation Loss: 0.49892 | Training Acc: 81.667 | validation Acc: 77.080\n",
            "Epoch 493: | Training Loss: 0.44420 | validation Loss: 0.49879 | Training Acc: 81.500 | validation Acc: 77.156\n",
            "Epoch 494: | Training Loss: 0.44344 | validation Loss: 0.49916 | Training Acc: 80.833 | validation Acc: 77.080\n",
            "Epoch 495: | Training Loss: 0.44583 | validation Loss: 0.50006 | Training Acc: 80.833 | validation Acc: 76.929\n",
            "Epoch 496: | Training Loss: 0.44690 | validation Loss: 0.49921 | Training Acc: 81.167 | validation Acc: 77.005\n",
            "Epoch 497: | Training Loss: 0.44161 | validation Loss: 0.49781 | Training Acc: 81.167 | validation Acc: 77.231\n",
            "Epoch 498: | Training Loss: 0.44461 | validation Loss: 0.49763 | Training Acc: 81.167 | validation Acc: 77.156\n",
            "Epoch 499: | Training Loss: 0.43722 | validation Loss: 0.49655 | Training Acc: 80.500 | validation Acc: 77.080\n",
            "Epoch 500: | Training Loss: 0.44264 | validation Loss: 0.49751 | Training Acc: 81.167 | validation Acc: 77.005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Loss Vs Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(train_loss_history, '-r')\n",
        "plt.plot(val_loss_history, '-b')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "K_4bzWfXDwhJ",
        "outputId": "516fc636-d39a-4925-b866-bc65293b9701"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1ee08f03d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 194
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Accuracy Vs Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(train_acc_history, '-r')\n",
        "plt.plot(val_acc_history, '-b')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "HL43as_3KzHU",
        "outputId": "a4746d51-793b-4c5a-f595-c6d672150612"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1ee0806f50>]"
            ]
          },
          "metadata": {},
          "execution_count": 187
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def get_accuracy_sensitivity_specificity(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    total = sum(sum(cm))\n",
        "    accuracy = (cm[0,0]+ cm[1,1])/total\n",
        "    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
        "    specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
        "    return accuracy, sensitivity, specificity\n",
        "\n",
        "#accuracy, sensitivity, specificity = get_accuracy_sensitivity_specificity(y_test, y_pred_list)\n",
        "#print(f'Accuracy = {accuracy}\\nSensitivity = {sensitivity}\\nSpecificity = {specificity}')"
      ],
      "metadata": {
        "id": "T2HjTTxAXJoJ"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_y_pred_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for k_X_batch in k_test_loader:\n",
        "        k_X_batch = k_X_batch.to(device)\n",
        "        k_y_test_pred = model(k_X_batch)\n",
        "        k_y_test_pred = torch.sigmoid(k_y_test_pred)\n",
        "        k_y_pred_tag = torch.round(k_y_test_pred)\n",
        "        k_y_pred_list.append(k_y_pred_tag.cpu().numpy())\n",
        "\n",
        "k_y_pred_list = [a.squeeze().tolist() for a in k_y_pred_list]"
      ],
      "metadata": {
        "id": "gFwbdKM3XNV0"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def say_true_false(prediction):\n",
        "    test_data_predection = []\n",
        "    for i in (prediction):\n",
        "        if (i > 0.5):\n",
        "            test_data_predection.append(True)\n",
        "        else:\n",
        "            test_data_predection.append(False)\n",
        "    return test_data_predection"
      ],
      "metadata": {
        "id": "pthkq5XQXXr8"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = say_true_false(k_y_pred_list)"
      ],
      "metadata": {
        "id": "POdzJIaUXcdf"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"PassengerId\":test_ids, \"Transported\":a })\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "5Zr-MqtRXfUb",
        "outputId": "4bb0219f-504c-4dca-8f3c-1accb46cd559"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     PassengerId  Transported\n",
              "0        0013_01         True\n",
              "1        0018_01        False\n",
              "2        0019_01         True\n",
              "3        0021_01         True\n",
              "4        0023_01         True\n",
              "...          ...          ...\n",
              "4272     9266_02         True\n",
              "4273     9269_01        False\n",
              "4274     9271_01         True\n",
              "4275     9273_01         True\n",
              "4276     9277_01         True\n",
              "\n",
              "[4277 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41cf698c-75fe-42e9-a5a5-abe93b73d4e9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Transported</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0013_01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0018_01</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0019_01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0021_01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0023_01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4272</th>\n",
              "      <td>9266_02</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4273</th>\n",
              "      <td>9269_01</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4274</th>\n",
              "      <td>9271_01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4275</th>\n",
              "      <td>9273_01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4276</th>\n",
              "      <td>9277_01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4277 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41cf698c-75fe-42e9-a5a5-abe93b73d4e9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-41cf698c-75fe-42e9-a5a5-abe93b73d4e9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-41cf698c-75fe-42e9-a5a5-abe93b73d4e9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "O0NpaqLSXild"
      },
      "execution_count": 193,
      "outputs": []
    }
  ]
}